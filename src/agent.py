import os
import json
import copy
import asyncio
from typing import Annotated, TypedDict, Optional

# LangGraph related
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

# Model related
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import ToolNode


# --- Tools that need automatic username injection ---
USER_INJECTED_TOOLS = {
    # File management tools
    "list_files", "read_file", "write_file", "append_file", "delete_file",
    # Command execution tools
    "run_command", "run_python_code",
    # Alarm management tools
    "add_alarm", "list_alarms", "delete_alarm",
    # Bark push notification tools
    "set_push_key", "send_push_notification", "get_push_status",
    "set_public_url", "get_public_url", "clear_public_url",
}


# --- State definition ---
class AgentState(TypedDict):
    messages: Annotated[list, add_messages]
    trigger_source: str
    enabled_tools: Optional[list[str]]
    user_id: Optional[str]
    session_id: Optional[str]
    # å¤–éƒ¨è°ƒç”¨æ–¹ä¼ å…¥çš„ tools å®šä¹‰ï¼ˆOpenAI function calling æ ¼å¼ï¼‰
    # å½“ LLM é€‰æ‹©è°ƒç”¨è¿™äº›å·¥å…·æ—¶ï¼Œä¸­æ–­å›¾æ‰§è¡Œå¹¶ä»¥ tool_calls æ ¼å¼è¿”å›žç»™è°ƒç”¨æ–¹
    external_tools: Optional[list[dict]]


class UserAwareToolNode:
    """
    Custom tool node:
    1. Reads thread_id from RunnableConfig, auto-injects as username for file/command tools
    2. Intercepts calls to disabled tools at runtime, returns error ToolMessage
    """
    def __init__(self, tools, get_mcp_tools_fn):
        self.tool_node = ToolNode(tools)
        self._get_mcp_tools = get_mcp_tools_fn

    async def __call__(self, state, config: RunnableConfig):
        # Get user_id directly from state (injected by mainagent) instead of
        # parsing thread_id, because user_id itself may contain the separator.
        user_id = state.get("user_id") or "anonymous"

        last_message = state["messages"][-1]
        if not hasattr(last_message, "tool_calls") or not last_message.tool_calls:
            return {"messages": []}

        # Get currently enabled tool set
        enabled_names = state.get("enabled_tools")
        if enabled_names is not None:
            enabled_set = set(enabled_names)
        else:
            enabled_set = None  # None = all allowed

        # Separate blocked and allowed calls
        modified_message = copy.deepcopy(last_message)
        blocked_calls = []
        allowed_calls = []
        for tc in modified_message.tool_calls:
            if enabled_set is not None and tc["name"] not in enabled_set:
                blocked_calls.append(tc)
                print(f">>> [tools] ðŸš« æ‹¦æˆªç¦ç”¨å·¥å…·è°ƒç”¨: {tc['name']}")
            else:
                if tc["name"] in USER_INJECTED_TOOLS:
                    tc["args"]["username"] = user_id
                # ç»™ add_alarm é¢å¤–æ³¨å…¥ session_idï¼Œè®©é—¹é’Ÿè®°ä½è®¾ç½®æ—¶çš„ä¼šè¯
                if tc["name"] == "add_alarm":
                    tc["args"]["session_id"] = state.get("session_id") or "default"
                allowed_calls.append(tc)
                print(f">>> [tools] âœ… è°ƒç”¨å·¥å…·: {tc['name']}")

        result_messages = []

        # For blocked tools, return error ToolMessages directly
        for tc in blocked_calls:
            result_messages.append(
                ToolMessage(
                    content=f"âŒ å·¥å…· '{tc['name']}' å½“å‰å·²è¢«ç¦ç”¨ã€‚è¿™é€šå¸¸æ˜¯ä¸ºäº†ä¿æŠ¤æ‚¨çš„ç³»ç»Ÿå®‰å…¨æˆ–ä¼˜åŒ–å½“å‰ä¼šè¯èµ„æºã€‚å¦‚æžœæ‚¨ç¡®å®žéœ€è¦æ­¤åŠŸèƒ½ï¼Œè¯·åœ¨ç®¡ç†é¢æ¿ä¸­å°†å…¶å¼€å¯ã€‚åŒæ—¶ï¼Œæ‚¨å¯ä»¥å‘Šè¯‰æˆ‘æ‚¨çš„æœ€ç»ˆç›®æ ‡ï¼Œæˆ‘ä¼šå°è¯•ç”¨å…¶ä»–å·²å¯ç”¨çš„å·¥å…·ä¸ºæ‚¨å¯»æ‰¾æ›¿ä»£æ–¹æ¡ˆã€‚",
                    tool_call_id=tc["id"],
                )
            )

        # For allowed tools, execute normally via ToolNode
        if allowed_calls:
            modified_message.tool_calls = allowed_calls
            modified_state = {**state, "messages": state["messages"][:-1] + [modified_message]}
            tool_result = await self.tool_node.ainvoke(modified_state, config)
            result_messages.extend(tool_result.get("messages", []))

        return {"messages": result_messages}


class MiniTimeAgent:
    """
    Encapsulates the full LangGraph agent: MCP tool loading, graph building,
    invoke/stream interface, task & tool-state management.
    """

    def __init__(self, src_dir: str, db_path: str):
        """
        Args:
            src_dir:  Path to src/ directory (where mcp_*.py live)
            db_path:  Path to SQLite checkpoint database
        """
        self._src_dir = src_dir
        self._db_path = db_path

        # Populated during startup
        self._mcp_tools: list = []
        self._agent_app = None
        self._mcp_client: Optional[MultiServerMCPClient] = None
        self._memory = None
        self._memory_ctx = None

        # Per-user state
        self._active_tasks: dict[str, asyncio.Task] = {}
        self._task_lock = asyncio.Lock()
        self._user_last_tool_state: dict[str, frozenset[str]] = {}

        # å¯åŠ¨æ—¶ä¸€æ¬¡æ€§åŠ è½½ prompt æ¨¡æ¿
        self._prompts = self._load_prompts()

    # ------------------------------------------------------------------
    # Prompt loader (å¯åŠ¨æ—¶è¯»å–ä¸€æ¬¡)
    # ------------------------------------------------------------------
    @staticmethod
    def _load_prompts() -> dict[str, str]:
        """ä»Ž data/prompts/ åŠ è½½æ‰€æœ‰ prompt æ¨¡æ¿æ–‡ä»¶ï¼ŒæœåŠ¡å¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡ã€‚"""
        prompts_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "prompts")
        prompt_files = {
            "base_system": "base_system.txt",
            "system_trigger": "system_trigger.txt",
            "tool_status": "tool_status.txt",
        }
        loaded = {}
        for key, filename in prompt_files.items():
            filepath = os.path.join(prompts_dir, filename)
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    loaded[key] = f.read().strip()
                print(f"[prompts] âœ… å·²åŠ è½½ {filename}")
            except FileNotFoundError:
                print(f"[prompts] âš ï¸ æœªæ‰¾åˆ° {filepath}ï¼Œå°†ä½¿ç”¨å†…ç½®é»˜è®¤å€¼")
                loaded[key] = ""

        # è®°å½• user_files æ ¹ç›®å½•è·¯å¾„ï¼ˆç”¨æˆ·ç”»åƒå­˜åœ¨å„ç”¨æˆ·ç›®å½•ä¸‹ï¼‰
        loaded["_user_files_dir"] = os.path.join(
            os.path.dirname(os.path.dirname(__file__)), "data", "user_files"
        )

        return loaded

    def _get_user_profile(self, user_id: str) -> str:
        """ä»Ž data/user_files/{user_id}/user_profile.txt è¯»å–ç”¨æˆ·ç”»åƒã€‚"""
        user_files_dir = self._prompts.get("_user_files_dir", "")
        fpath = os.path.join(user_files_dir, user_id, "user_profile.txt")
        try:
            with open(fpath, "r", encoding="utf-8") as f:
                return f.read().strip()
        except FileNotFoundError:
            return ""

    def _get_user_skills(self, user_id: str) -> str:
        """
        ä»Ž data/user_files/{user_id}/skills_manifest.json è¯»å–ç”¨æˆ·çš„ skill listï¼Œ
        å¹¶è¿”å›žæ ¼å¼åŒ–çš„ skill ä¿¡æ¯å­—ç¬¦ä¸²ã€‚
        å³ä½¿æ²¡æœ‰ skillï¼Œä¹Ÿä¼šè¿”å›žä½ç½®ä¿¡æ¯ã€‚
        """
        user_files_dir = self._prompts.get("_user_files_dir", "")
        manifest_path = os.path.join(user_files_dir, user_id, "skills_manifest.json")
        skills_dir = os.path.join(user_files_dir, user_id, "skills")

        skills_manifest = []
        try:
            with open(manifest_path, "r", encoding="utf-8") as f:
                skills_manifest = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            pass

        # æ ¼å¼åŒ– skill ä¿¡æ¯ï¼ˆå³ä½¿ä¸ºç©ºä¹Ÿè¿”å›žä½ç½®ä¿¡æ¯ï¼‰
        skill_lines = ["\nã€ç”¨æˆ·æŠ€èƒ½åˆ—è¡¨ã€‘"]
        skill_lines.append(f"æŠ€èƒ½æ¸…å•æ–‡ä»¶ä½ç½®: {manifest_path}")
        skill_lines.append(f"æŠ€èƒ½æ–‡ä»¶ç›®å½•ä½ç½®: {skills_dir}")

        if skills_manifest:
            skill_lines.append("å¯ç”¨æŠ€èƒ½ï¼š")
            for skill in skills_manifest:
                skill_name = skill.get("name", "æœªå‘½åæŠ€èƒ½")
                skill_desc = skill.get("description", "æ— æè¿°")
                skill_file = skill.get("file", "")
                skill_lines.append(f"  - {skill_name}: {skill_desc}")
                if skill_file:
                    skill_lines.append(f"    æ–‡ä»¶: {os.path.join(skills_dir, skill_file)}")
            skill_lines.append("å¦‚éœ€ä½¿ç”¨æŸä¸ªæŠ€èƒ½ï¼Œè¯·ä½¿ç”¨æ–‡ä»¶ç®¡ç†å·¥å…·è¯»å–å¯¹åº”çš„æŠ€èƒ½æ–‡ä»¶ã€‚")
        else:
            skill_lines.append("å½“å‰æš‚æ— å·²æ³¨å†Œçš„æŠ€èƒ½ã€‚")
            skill_lines.append("å¦‚éœ€æ·»åŠ æŠ€èƒ½ï¼Œè¯·åœ¨æŠ€èƒ½æ¸…å•æ–‡ä»¶ä¸­æ·»åŠ æŠ€èƒ½ä¿¡æ¯ã€‚")

        return "\n".join(skill_lines)

    # ------------------------------------------------------------------
    # Properties
    # ------------------------------------------------------------------
    @property
    def mcp_tools(self) -> list:
        return self._mcp_tools

    @property
    def agent_app(self):
        return self._agent_app

    # ------------------------------------------------------------------
    # Lifecycle
    # ------------------------------------------------------------------
    async def startup(self):
        """Initialize MCP client, load tools, build LangGraph workflow."""
        # 1. Open checkpoint DB
        self._memory_ctx = AsyncSqliteSaver.from_conn_string(self._db_path)
        self._memory = await self._memory_ctx.__aenter__()

        # 2. Start MCP servers
        self._mcp_client = MultiServerMCPClient({
            "scheduler_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_scheduler.py")],
                "transport": "stdio",
            },
            "search_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_search.py")],
                "transport": "stdio",
            },
            "file_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_filemanager.py")],
                "transport": "stdio",
            },
            "commander_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_commander.py")],
                "transport": "stdio",
            },
            "oasis_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_oasis.py")],
                "transport": "stdio",
            },
            "bark_service": {
                "command": "python",
                "args": [os.path.join(self._src_dir, "mcp_bark.py")],
                "transport": "stdio",
            },
        })

        # 3. Fetch tool definitions (new API: no context manager needed)
        self._mcp_tools = await self._mcp_client.get_tools()

        # 4. Build LangGraph workflow
        # æ”¶é›†æ‰€æœ‰å†…éƒ¨ MCP å·¥å…·åç§°ï¼Œç”¨äºŽæ¡ä»¶è·¯ç”±
        self._internal_tool_names = frozenset(t.name for t in self._mcp_tools)

        workflow = StateGraph(AgentState)
        workflow.add_node("chatbot", self._call_model)
        workflow.add_node("tools", UserAwareToolNode(self._mcp_tools, lambda: self._mcp_tools))
        workflow.add_edge(START, "chatbot")
        workflow.add_conditional_edges("chatbot", self._should_continue)
        workflow.add_edge("tools", "chatbot")

        self._agent_app = workflow.compile(checkpointer=self._memory)
        print("--- Agent æœåŠ¡å·²å¯åŠ¨ï¼Œå¤–éƒ¨å®šæ—¶/ç”¨æˆ·è¾“å…¥åŒå…¼å®¹å°±ç»ª ---")

    async def shutdown(self):
        """Clean up MCP client and checkpoint DB."""
        if self._memory_ctx:
            try:
                await self._memory_ctx.__aexit__(None, None, None)
            except Exception:
                pass

    # ------------------------------------------------------------------
    # Model factory
    # ------------------------------------------------------------------
    @staticmethod
    def _get_model() -> ChatOpenAI:
        api_key = os.getenv("LLM_API_KEY")
        base_url = os.getenv("LLM_BASE_URL", "https://api.deepseek.com/v1")
        model = os.getenv("LLM_MODEL", "deepseek-chat")
        if not api_key:
            raise ValueError("æœªæ£€æµ‹åˆ° LLM_API_KEYï¼Œè¯·åœ¨çŽ¯å¢ƒå˜é‡ä¸­è®¾ç½®ã€‚")
        return ChatOpenAI(
            model=model,
            base_url=base_url,
            api_key=api_key,
            temperature=0.7,
            max_tokens=2048,
            timeout=60,
            max_retries=2,
        )

    # ------------------------------------------------------------------
    # Conditional edge: route internal tools vs external tools vs end
    # ------------------------------------------------------------------
    def _should_continue(self, state: AgentState) -> str:
        """
        æ¡ä»¶è·¯ç”±ï¼š
        - æ—  tool_calls â†’ "end" (æ­£å¸¸ç»“æŸ)
        - æ‰€æœ‰ tool_calls éƒ½æ˜¯å†…éƒ¨å·¥å…· â†’ "tools" (ç»§ç»­å†…éƒ¨å¾ªçŽ¯)
        - å­˜åœ¨å¤–éƒ¨å·¥å…·è°ƒç”¨ â†’ "end" (ä¸­æ–­è¿”å›ž tool_calls ç»™è°ƒç”¨æ–¹)
        """
        last_msg = state["messages"][-1]
        if not hasattr(last_msg, "tool_calls") or not last_msg.tool_calls:
            return END

        for tc in last_msg.tool_calls:
            if tc["name"] not in self._internal_tool_names:
                # å‘çŽ°å¤–éƒ¨å·¥å…·è°ƒç”¨ï¼Œä¸­æ–­å¾ªçŽ¯è®©è°ƒç”¨æ–¹å¤„ç†
                print(f">>> [route] ðŸ”€ å¤–éƒ¨å·¥å…·è°ƒç”¨æ£€æµ‹: {tc['name']}ï¼Œä¸­æ–­è¿”å›žç»™è°ƒç”¨æ–¹")
                return END
        return "tools"

    # ------------------------------------------------------------------
    # Core graph node
    # ------------------------------------------------------------------
    async def _call_model(self, state: AgentState):
        """LangGraph node: invoke LLM with dynamic tool binding & tool-state notification."""

        # Dynamic tool binding based on enabled_tools + external_tools
        all_tools = self._mcp_tools
        enabled_names = state.get("enabled_tools")
        if enabled_names is not None:
            filtered_tools = [t for t in all_tools if t.name in enabled_names]
        else:
            filtered_tools = all_tools

        # å°†å¤–éƒ¨å·¥å…·å®šä¹‰ï¼ˆOpenAI function formatï¼‰è½¬ä¸º LangChain å¯ç»‘å®šçš„æ ¼å¼
        external_tools_defs = state.get("external_tools") or []
        bind_tools_list: list = list(filtered_tools)
        external_tool_names: set[str] = set()
        for ext_tool in external_tools_defs:
            # æ”¯æŒ OpenAI æ ‡å‡†æ ¼å¼: {"type":"function","function":{...}} æˆ–ç®€åŒ–æ ¼å¼ {"name":...,"parameters":...}
            if ext_tool.get("type") == "function":
                func_def = ext_tool.get("function", {})
            else:
                func_def = ext_tool
            if func_def.get("name"):
                external_tool_names.add(func_def["name"])
                # ä»¥ OpenAI function æ ¼å¼ä¼ å…¥ bind_toolsï¼ˆLangChain æ”¯æŒ dict æ ¼å¼ï¼‰
                bind_tools_list.append({
                    "type": "function",
                    "function": {
                        "name": func_def["name"],
                        "description": func_def.get("description", ""),
                        "parameters": func_def.get("parameters", {"type": "object", "properties": {}}),
                    },
                })

        base_model = self._get_model()
        llm = base_model.bind_tools(bind_tools_list) if bind_tools_list else base_model

        # --- KV-Cache-friendly tool state management ---
        all_names = sorted(t.name for t in all_tools)
        all_tool_list_str = ", ".join(all_names)

        base_prompt = (
            self._prompts["base_system"] + "\n\n"
            f"ã€é»˜è®¤å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘\n{all_tool_list_str}\n"
            "ä»¥ä¸Šå·¥å…·é»˜è®¤å…¨éƒ¨å¯ç”¨ã€‚å¦‚æžœåŽç»­æœ‰å·¥å…·çŠ¶æ€å˜æ›´ï¼Œç³»ç»Ÿä¼šå¦è¡Œé€šçŸ¥ã€‚\n"
        )

        # Detect tool state change
        current_enabled = frozenset(enabled_names) if enabled_names is not None else frozenset(all_names)
        user_id = state.get("user_id", "__global__")

        # æ³¨å…¥ç”¨æˆ·ä¸“å±žç”»åƒ
        user_profile = self._get_user_profile(user_id)
        if user_profile:
            base_prompt += f"\n{user_profile}\n"

        # æ³¨å…¥ç”¨æˆ·æŠ€èƒ½åˆ—è¡¨ï¼ˆæ€»æ˜¯æ˜¾ç¤ºä½ç½®ä¿¡æ¯ï¼‰
        base_prompt += self._get_user_skills(user_id) + "\n"

        last_state = self._user_last_tool_state.get(user_id)

        tool_status_prompt = ""
        if last_state is not None and current_enabled != last_state:
            all_names_set = set(all_names)
            enabled_set = set(current_enabled)
            disabled_names_set = all_names_set - enabled_set
            tool_status_prompt = self._prompts["tool_status"].format(
                enabled_tools=', '.join(sorted(enabled_set & all_names_set)) if (enabled_set & all_names_set) else 'æ— ',
                disabled_tools=', '.join(sorted(disabled_names_set)) if disabled_names_set else 'æ— ',
            )
        elif last_state is None and enabled_names is not None:
            all_names_set = set(all_names)
            enabled_set = set(current_enabled)
            disabled_names_set = all_names_set - enabled_set
            if disabled_names_set:
                tool_status_prompt = self._prompts["tool_status"].format(
                    enabled_tools=', '.join(sorted(enabled_set & all_names_set)) if (enabled_set & all_names_set) else 'æ— ',
                    disabled_tools=', '.join(sorted(disabled_names_set)),
                )

        # Update cache
        self._user_last_tool_state[user_id] = current_enabled

        history_messages = list(state["messages"])

        # æ¯æ¬¡è¿›å…¥å‰æ¸…ç†ï¼šç§»é™¤æœ«å°¾ä¸å®Œæ•´çš„ tool_callsï¼ˆæœ‰ AIMessage å¸¦ tool_calls ä½†ç¼ºå°‘ ToolMessage å›žå¤ï¼‰
        # ä½†ä¿ç•™å¤–éƒ¨å·¥å…·çš„æœªå›žå¤ tool_callsï¼ˆå®ƒä»¬æ­£ç­‰å¾…è°ƒç”¨æ–¹å›žä¼ ç»“æžœï¼‰
        history_messages = self._sanitize_messages(history_messages, external_tool_names)

        # æ¸…ç†åŽ†å²æ¶ˆæ¯ä¸­çš„å¤šæ¨¡æ€å†…å®¹ï¼ˆfile/image/audio partsï¼‰ï¼Œåªä¿ç•™æ–‡æœ¬
        # é¿å…æ—§çš„äºŒè¿›åˆ¶é™„ä»¶åœ¨åŽç»­è½®æ¬¡åå¤å‘é€ç»™ LLM å¯¼è‡´ä¸Šæ¸¸ API æŠ¥é”™
        # æ³¨æ„ï¼šä¿ç•™æœ€åŽä¸€æ¡ HumanMessage çš„å¤šæ¨¡æ€å†…å®¹ï¼ˆå½“å‰è½®ç”¨æˆ·è¾“å…¥ï¼‰
        if len(history_messages) > 1:
            history_messages = self._strip_multimodal_parts(history_messages[:-1]) + [history_messages[-1]]

        # å¦‚æžœæ˜¯ç³»ç»Ÿè§¦å‘ï¼Œä¸”æœ€åŽä¸€æ¡ä¸æ˜¯ ToolMessageï¼ˆéžå·¥å…·å›žè°ƒè½®ï¼‰ï¼Œç»™å®ƒåŠ ä¸Šç³»ç»Ÿè§¦å‘è¯´æ˜Ž
        is_system = state.get("trigger_source") == "system"
        if is_system and history_messages and isinstance(history_messages[-1], HumanMessage):
            original_text = history_messages[-1].content
            system_trigger_prompt = self._prompts["system_trigger"].format(
                original_text=original_text
            )
            history_messages = history_messages[:-1] + [HumanMessage(content=system_trigger_prompt)]

        # æ­£å¸¸å¯¹è¯æµç¨‹ï¼ˆç”¨æˆ·å’Œç³»ç»Ÿè§¦å‘å…±ç”¨ï¼‰
        if tool_status_prompt and len(history_messages) >= 1:
            last_msg = history_messages[-1]
            # å¦‚æžœæœ€åŽä¸€æ¡æ˜¯å¤šæ¨¡æ€ contentï¼ˆlistï¼‰ï¼Œå°†é€šçŸ¥æ’å…¥ä¸ºç¬¬ä¸€ä¸ª text part
            if isinstance(last_msg.content, list):
                notification = {"type": "text", "text": f"[ç³»ç»Ÿé€šçŸ¥] {tool_status_prompt}\n\n---\n"}
                augmented_content = [notification] + list(last_msg.content)
                augmented_msg = HumanMessage(content=augmented_content)
            else:
                augmented_content = f"[ç³»ç»Ÿé€šçŸ¥] {tool_status_prompt}\n\n---\n{last_msg.content}"
                augmented_msg = HumanMessage(content=augmented_content)
            input_messages = (
                [SystemMessage(content=base_prompt)]
                + history_messages[:-1]
                + [augmented_msg]
            )
        else:
            input_messages = [SystemMessage(content=base_prompt)] + history_messages

        response = await llm.ainvoke(input_messages)
        return {"messages": [response]}

    # ------------------------------------------------------------------
    # Public interface: tools info
    # ------------------------------------------------------------------
    @staticmethod
    def _sanitize_messages(messages: list, external_tool_names: set[str] | None = None) -> list:
        """
        æ¸…ç†æ¶ˆæ¯åˆ—è¡¨ï¼Œç¡®ä¿æ¯æ¡å¸¦ tool_calls çš„ AI æ¶ˆæ¯åŽé¢éƒ½æœ‰å¯¹åº”çš„ ToolMessageã€‚
        å¦‚æžœæœ«å°¾æœ‰ä¸å®Œæ•´çš„ tool_calls åºåˆ—ï¼Œç›´æŽ¥æˆªæ–­ä¸¢å¼ƒã€‚

        ä½†å¦‚æžœ external_tool_names éžç©ºï¼Œåˆ™ä¿ç•™æœ«å°¾ AIMessage ä¸­å±žäºŽå¤–éƒ¨å·¥å…·çš„
        æœªå›žå¤ tool_callsï¼ˆå®ƒä»¬æ­£ç­‰å¾…è°ƒç”¨æ–¹å›žä¼ ç»“æžœï¼‰ã€‚
        """
        if not external_tool_names:
            external_tool_names = set()

        # æ”¶é›†æ‰€æœ‰å·²å­˜åœ¨çš„ tool_call_id å›žå¤
        answered_ids = set()
        for msg in messages:
            if isinstance(msg, ToolMessage) and hasattr(msg, "tool_call_id"):
                answered_ids.add(msg.tool_call_id)

        # ä»ŽåŽå¾€å‰æ‰¾åˆ°ç¬¬ä¸€ä¸ª"å®Œæ•´"çš„ä½ç½®
        clean = list(messages)
        while clean:
            last = clean[-1]
            # å¦‚æžœæœ€åŽä¸€æ¡æ˜¯å¸¦ tool_calls çš„ AI æ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦å…¨éƒ¨æœ‰å›žå¤
            if isinstance(last, AIMessage) and hasattr(last, "tool_calls") and last.tool_calls:
                pending_ids = {tc["id"] for tc in last.tool_calls}
                if not pending_ids.issubset(answered_ids):
                    # æ£€æŸ¥æœªå›žå¤çš„ tool_calls æ˜¯å¦å…¨éƒ¨å±žäºŽå¤–éƒ¨å·¥å…·
                    unanswered_calls = [
                        tc for tc in last.tool_calls if tc["id"] not in answered_ids
                    ]
                    all_external = all(
                        tc["name"] in external_tool_names for tc in unanswered_calls
                    )
                    if all_external and external_tool_names:
                        # å¤–éƒ¨å·¥å…·ç­‰å¾…å›žä¼ ï¼Œä¿ç•™æ­¤æ¶ˆæ¯
                        break
                    # å†…éƒ¨å·¥å…·æœªå®Œæˆ â†’ æˆªæ–­
                    clean.pop()
                    continue
            break
        return clean

    @staticmethod
    def _strip_multimodal_parts(messages: list) -> list:
        """
        å°†æ‰€æœ‰ HumanMessage ä¸­çš„å¤šæ¨¡æ€ contentï¼ˆlist æ ¼å¼ï¼‰è½¬ä¸ºçº¯æ–‡æœ¬ã€‚
        - type:"text" çš„ part ä¿ç•™æ–‡æœ¬
        - type:"file" æ›¿æ¢ä¸º "[ç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶: {filename}]"
        - type:"image_url" æ›¿æ¢ä¸º "[ç”¨æˆ·ä¸Šä¼ äº†å›¾ç‰‡]"
        - å…¶ä»–æœªçŸ¥ type ä¸¢å¼ƒ
        """
        result = []
        for msg in messages:
            if isinstance(msg, HumanMessage) and isinstance(msg.content, list):
                text_parts = []
                for part in msg.content:
                    if not isinstance(part, dict):
                        text_parts.append(str(part))
                        continue
                    ptype = part.get("type", "")
                    if ptype == "text":
                        text_parts.append(part.get("text", ""))
                    elif ptype == "file":
                        fname = part.get("file", {}).get("filename", "é™„ä»¶")
                        text_parts.append(f"[ç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶: {fname}]")
                    elif ptype == "image_url":
                        text_parts.append("[ç”¨æˆ·ä¸Šä¼ äº†å›¾ç‰‡]")
                combined = "\n".join(t for t in text_parts if t)
                result.append(HumanMessage(content=combined or "(ç©ºæ¶ˆæ¯)"))
            else:
                result.append(msg)
        return result

    def get_tools_info(self) -> list[dict]:
        """Return serializable tool metadata list."""
        return [{"name": t.name, "description": t.description or ""} for t in self._mcp_tools]

    # ------------------------------------------------------------------
    # Public interface: task management
    # ------------------------------------------------------------------
    async def cancel_task(self, user_id: str):
        """Cancel the active streaming task for a user."""
        async with self._task_lock:
            task = self._active_tasks.get(user_id)
            if task and not task.done():
                task.cancel()
                try:
                    await asyncio.wait_for(asyncio.shield(task), timeout=5.0)
                except (asyncio.CancelledError, asyncio.TimeoutError, Exception):
                    pass
            self._active_tasks.pop(user_id, None)

    def register_task(self, user_id: str, task: asyncio.Task):
        """Register an active streaming task for a user."""
        self._active_tasks[user_id] = task

    def unregister_task(self, user_id: str):
        """Remove a finished task from the registry."""
        self._active_tasks.pop(user_id, None)
