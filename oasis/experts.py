"""
OASIS Forum - Expert Agent definitions

Two expert backends:
  1. ExpertAgent  â€” direct LLM call (stateless, single-shot, original behavior)
  2. BotSessionExpert â€” calls mini_timebot's own /v1/chat/completions endpoint,
     each expert gets an isolated temporary session with full tool-calling ability.
     Sessions are created on demand and cleaned up after the discussion ends.

Each expert participates in forum discussions by reading others' posts,
publishing their own views, and voting.
"""

import json
import os
import sys

import httpx
from langchain_core.messages import HumanMessage

# ç¡®ä¿ src/ åœ¨ import è·¯å¾„ä¸­ï¼Œä»¥ä¾¿å¯¼å…¥ llm_factory
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)), "src"))
from llm_factory import create_chat_model, extract_text

from oasis.forum import DiscussionForum


# --- åŠ è½½ prompt å’Œä¸“å®¶é…ç½®ï¼ˆæ¨¡å—çº§åˆ«ï¼Œå¯¼å…¥æ—¶æ‰§è¡Œä¸€æ¬¡ï¼‰ ---
_data_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data")
_prompts_dir = os.path.join(_data_dir, "prompts")

# åŠ è½½å…¬å…±ä¸“å®¶é…ç½®
_experts_json_path = os.path.join(_prompts_dir, "oasis_experts.json")
try:
    with open(_experts_json_path, "r", encoding="utf-8") as f:
        EXPERT_CONFIGS: list[dict] = json.load(f)
    print(f"[prompts] âœ… oasis å·²åŠ è½½ oasis_experts.json ({len(EXPERT_CONFIGS)} ä½å…¬å…±ä¸“å®¶)")
except FileNotFoundError:
    print(f"[prompts] âš ï¸ æœªæ‰¾åˆ° {_experts_json_path}ï¼Œä½¿ç”¨å†…ç½®é»˜è®¤é…ç½®")
    EXPERT_CONFIGS = [
        {"name": "åˆ›æ„ä¸“å®¶", "tag": "creative", "persona": "ä½ æ˜¯ä¸€ä¸ªä¹è§‚çš„åˆ›æ–°è€…ï¼Œå–„äºå‘ç°æœºé‡å’Œéå¸¸è§„è§£å†³æ–¹æ¡ˆã€‚ä½ å–œæ¬¢æŒ‘æˆ˜ä¼ ç»Ÿè§‚å¿µï¼Œæå‡ºå¤§èƒ†ä¸”å…·æœ‰å‰ç»æ€§çš„æƒ³æ³•ã€‚", "temperature": 0.9},
        {"name": "æ‰¹åˆ¤ä¸“å®¶", "tag": "critical", "persona": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ‰¹åˆ¤æ€§æ€è€ƒè€…ï¼Œå–„äºå‘ç°é£é™©ã€æ¼æ´å’Œé€»è¾‘è°¬è¯¯ã€‚ä½ ä¼šæŒ‡å‡ºæ–¹æ¡ˆä¸­çš„æ½œåœ¨é—®é¢˜ï¼Œç¡®ä¿è®¨è®ºä¸ä¼šå¿½è§†é‡è¦ç»†èŠ‚ã€‚", "temperature": 0.3},
        {"name": "æ•°æ®åˆ†æå¸ˆ", "tag": "data", "persona": "ä½ æ˜¯ä¸€ä¸ªæ•°æ®é©±åŠ¨çš„åˆ†æå¸ˆï¼Œåªç›¸ä¿¡æ•°æ®å’Œäº‹å®ã€‚ä½ ç”¨æ•°å­—ã€æ¡ˆä¾‹å’Œé€»è¾‘æ¨å¯¼æ¥æ”¯æ’‘ä½ çš„è§‚ç‚¹ã€‚", "temperature": 0.5},
        {"name": "ç»¼åˆé¡¾é—®", "tag": "synthesis", "persona": "ä½ å–„äºç»¼åˆä¸åŒè§‚ç‚¹ï¼Œå¯»æ‰¾å¹³è¡¡æ–¹æ¡ˆï¼Œå…³æ³¨å®é™…å¯æ“ä½œæ€§ã€‚ä½ ä¼šè¯†åˆ«å„æ–¹å…±è¯†ï¼Œæå‡ºå…¼é¡¾å¤šæ–¹åˆ©ç›Šçš„åŠ¡å®å»ºè®®ã€‚", "temperature": 0.5},
    ]


# ======================================================================
# Per-user custom expert storage
# ======================================================================
_USER_EXPERTS_DIR = os.path.join(_data_dir, "oasis_user_experts")
os.makedirs(_USER_EXPERTS_DIR, exist_ok=True)


def _user_experts_path(user_id: str) -> str:
    """Return the JSON file path for a user's custom experts."""
    safe = user_id.replace("/", "_").replace("\\", "_").replace("..", "_")
    return os.path.join(_USER_EXPERTS_DIR, f"{safe}.json")


def load_user_experts(user_id: str) -> list[dict]:
    """Load a user's custom expert list (returns [] if none)."""
    path = _user_experts_path(user_id)
    if not os.path.exists(path):
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except (json.JSONDecodeError, OSError):
        return []


def _save_user_experts(user_id: str, experts: list[dict]) -> None:
    with open(_user_experts_path(user_id), "w", encoding="utf-8") as f:
        json.dump(experts, f, ensure_ascii=False, indent=2)


def _validate_expert(data: dict) -> dict:
    """Validate and normalize an expert config dict. Raises ValueError on bad input."""
    name = data.get("name", "").strip()
    tag = data.get("tag", "").strip()
    persona = data.get("persona", "").strip()
    if not name:
        raise ValueError("ä¸“å®¶ name ä¸èƒ½ä¸ºç©º")
    if not tag:
        raise ValueError("ä¸“å®¶ tag ä¸èƒ½ä¸ºç©º")
    if not persona:
        raise ValueError("ä¸“å®¶ persona ä¸èƒ½ä¸ºç©º")
    return {
        "name": name,
        "tag": tag,
        "persona": persona,
        "temperature": float(data.get("temperature", 0.7)),
    }


def add_user_expert(user_id: str, data: dict) -> dict:
    """Add a custom expert for a user. Returns the normalized expert dict."""
    expert = _validate_expert(data)
    experts = load_user_experts(user_id)
    # Prevent duplicate tag within user's list
    if any(e["tag"] == expert["tag"] for e in experts):
        raise ValueError(f"ç”¨æˆ·å·²æœ‰ tag=\"{expert['tag']}\" çš„ä¸“å®¶ï¼Œè¯·æ¢ä¸€ä¸ª tag æˆ–ä½¿ç”¨æ›´æ–°åŠŸèƒ½")
    # Prevent clash with global expert tags
    if any(e["tag"] == expert["tag"] for e in EXPERT_CONFIGS):
        raise ValueError(f"tag=\"{expert['tag']}\" ä¸å…¬å…±ä¸“å®¶å†²çªï¼Œè¯·æ¢ä¸€ä¸ª tag")
    experts.append(expert)
    _save_user_experts(user_id, experts)
    return expert


def update_user_expert(user_id: str, tag: str, data: dict) -> dict:
    """Update an existing custom expert by tag. Returns the updated dict."""
    experts = load_user_experts(user_id)
    for i, e in enumerate(experts):
        if e["tag"] == tag:
            updated = _validate_expert({**e, **data, "tag": tag})  # tag immutable
            experts[i] = updated
            _save_user_experts(user_id, experts)
            return updated
    raise ValueError(f"æœªæ‰¾åˆ°ç”¨æˆ·è‡ªå®šä¹‰ä¸“å®¶ tag=\"{tag}\"")


def delete_user_expert(user_id: str, tag: str) -> dict:
    """Delete a custom expert by tag. Returns the deleted dict."""
    experts = load_user_experts(user_id)
    for i, e in enumerate(experts):
        if e["tag"] == tag:
            deleted = experts.pop(i)
            _save_user_experts(user_id, experts)
            return deleted
    raise ValueError(f"æœªæ‰¾åˆ°ç”¨æˆ·è‡ªå®šä¹‰ä¸“å®¶ tag=\"{tag}\"")


def get_all_experts(user_id: str | None = None) -> list[dict]:
    """Return public experts + user's custom experts (marked with source)."""
    result = [
        {**c, "source": "public"} for c in EXPERT_CONFIGS
    ]
    if user_id:
        result.extend(
            {**c, "source": "custom"} for c in load_user_experts(user_id)
        )
    return result

# åŠ è½½è®¨è®º prompt æ¨¡æ¿
_discuss_tpl_path = os.path.join(_prompts_dir, "oasis_expert_discuss.txt")
try:
    with open(_discuss_tpl_path, "r", encoding="utf-8") as f:
        _DISCUSS_PROMPT_TPL = f.read().strip()
    print("[prompts] âœ… oasis å·²åŠ è½½ oasis_expert_discuss.txt")
except FileNotFoundError:
    print(f"[prompts] âš ï¸ æœªæ‰¾åˆ° {_discuss_tpl_path}ï¼Œä½¿ç”¨å†…ç½®é»˜è®¤æ¨¡æ¿")
    _DISCUSS_PROMPT_TPL = ""


def _get_llm(temperature: float = 0.7):
    """Create an LLM instance (reuses the same env config & vendor routing as main agent)."""
    return create_chat_model(temperature=temperature, max_tokens=1024)


# ======================================================================
# Helper: build discussion prompt (shared by both backends)
# ======================================================================

def _build_discuss_prompt(
    expert_name: str,
    persona: str,
    question: str,
    posts_text: str,
) -> str:
    """Build the prompt that asks the expert to respond with JSON."""
    if _DISCUSS_PROMPT_TPL:
        return _DISCUSS_PROMPT_TPL.format(
            expert_name=expert_name,
            persona=persona,
            question=question,
            posts_text=posts_text,
        )
    return (
        f"ä½ æ˜¯è®ºå›ä¸“å®¶ã€Œ{expert_name}ã€ã€‚{persona}\n\n"
        f"è®¨è®ºä¸»é¢˜: {question}\n\n"
        f"å½“å‰è®ºå›å†…å®¹:\n{posts_text}\n\n"
        "è¯·ä»¥ä¸¥æ ¼çš„ JSON æ ¼å¼å›å¤ï¼ˆä¸è¦åŒ…å« markdown ä»£ç å—æ ‡è®°ï¼Œä¸è¦åŒ…å«æ³¨é‡Šï¼‰:\n"
        "{\n"
        '  "reply_to": 2,\n'
        '  "content": "ä½ çš„è§‚ç‚¹ï¼ˆ200å­—ä»¥å†…ï¼Œè§‚ç‚¹é²œæ˜ï¼‰",\n'
        '  "votes": [\n'
        '    {"post_id": 1, "direction": "up"}\n'
        "  ]\n"
        "}\n\n"
        "è¯´æ˜:\n"
        "- reply_to: å¦‚æœè®ºå›ä¸­å·²æœ‰å…¶ä»–äººçš„å¸–å­ï¼Œä½ **å¿…é¡»**é€‰æ‹©ä¸€ä¸ªå¸–å­IDè¿›è¡Œå›å¤ï¼›åªæœ‰åœ¨è®ºå›ä¸ºç©ºæ—¶æ‰å¡« null\n"
        "- content: ä½ çš„å‘è¨€å†…å®¹ï¼Œè¦æœ‰ç‹¬åˆ°è§è§£ï¼Œå¯ä»¥èµåŒã€åé©³æˆ–è¡¥å……ä½ æ‰€å›å¤çš„å¸–å­\n"
        '- votes: å¯¹å…¶ä»–å¸–å­çš„æŠ•ç¥¨åˆ—è¡¨ï¼Œdirection åªèƒ½æ˜¯ "up" æˆ– "down"ã€‚å¦‚æœæ²¡æœ‰è¦æŠ•ç¥¨çš„å¸–å­ï¼Œå¡«ç©ºåˆ—è¡¨ []\n'
    )


def _format_posts(posts) -> str:
    """Format posts for display in the prompt."""
    lines = []
    for p in posts:
        prefix = f"  â†³ å›å¤#{p.reply_to}" if p.reply_to else "ğŸ“Œ"
        lines.append(
            f"{prefix} [#{p.id}] {p.author} "
            f"(ğŸ‘{p.upvotes} ğŸ‘{p.downvotes}): {p.content}"
        )
    return "\n".join(lines)


def _parse_expert_response(raw: str):
    """Strip markdown fences and parse JSON. Returns dict or None."""
    raw = raw.strip()
    if raw.startswith("```"):
        raw = raw.split("\n", 1)[-1]
    if raw.endswith("```"):
        raw = raw.rsplit("```", 1)[0]
    raw = raw.strip()
    return json.loads(raw)


async def _apply_response(
    result: dict,
    expert_name: str,
    forum: DiscussionForum,
    others: list,
):
    """Apply the parsed JSON response: publish post + cast votes."""
    reply_to = result.get("reply_to")
    if reply_to is None and others:
        reply_to = others[-1].id
        print(f"  [OASIS] ğŸ”§ {expert_name} reply_to ä¸º nullï¼Œè‡ªåŠ¨è®¾ä¸º #{reply_to}")

    await forum.publish(
        author=expert_name,
        content=result.get("content", "ï¼ˆå‘è¨€å†…å®¹ä¸ºç©ºï¼‰"),
        reply_to=reply_to,
    )

    for v in result.get("votes", []):
        pid = v.get("post_id")
        direction = v.get("direction", "up")
        if pid is not None and direction in ("up", "down"):
            await forum.vote(expert_name, int(pid), direction)

    print(f"  [OASIS] âœ… {expert_name} å‘è¨€å®Œæˆ")


# ======================================================================
# Backend 1: ExpertAgent â€” direct LLM call (original, stateless)
# ======================================================================

class ExpertAgent:
    """
    A forum-resident expert agent (direct LLM backend).

    Each call is stateless: reads posts â†’ single LLM call â†’ publish + vote.
    """

    def __init__(self, name: str, persona: str, temperature: float = 0.7):
        self.name = name
        self.persona = persona
        self.llm = _get_llm(temperature)

    async def participate(self, forum: DiscussionForum):
        others = await forum.browse(viewer=self.name, exclude_self=True)
        posts_text = _format_posts(others) if others else "(è¿˜æ²¡æœ‰å…¶ä»–äººå‘è¨€ï¼Œä½ æ¥å¼€å¯è®¨è®ºå§)"
        prompt = _build_discuss_prompt(self.name, self.persona, forum.question, posts_text)

        try:
            resp = await self.llm.ainvoke([HumanMessage(content=prompt)])
            text = extract_text(resp.content)
            result = _parse_expert_response(text)
            await _apply_response(result, self.name, forum, others)
        except json.JSONDecodeError as e:
            print(f"  [OASIS] âš ï¸ {self.name} JSON parse error: {e}")
            try:
                await forum.publish(author=self.name, content=extract_text(resp.content).strip()[:300])
            except Exception:
                pass
        except Exception as e:
            print(f"  [OASIS] âŒ {self.name} error: {e}")


# ======================================================================
# Backend 2: BotSessionExpert â€” calls mini_timebot /v1/chat/completions
# ======================================================================

class BotSessionExpert:
    """
    Expert backed by a mini_timebot session.

    Each expert gets a unique session_id derived from the topic_id, **owned by
    the requesting user** (thread_id = ``{user_id}#oasis_{topic_id}_{expert}``).
    This means the expert's conversation history is visible in the user's
    session list and is **not** auto-deleted after the discussion â€” the user
    can revisit or continue chatting with any expert later.

    Authentication uses ``INTERNAL_TOKEN:<user_id>`` (admin-level), so no
    user password is needed â€” OASIS acts as a trusted internal service.

    **Incremental context**: To avoid O(NÂ²) token blowup, only NEW posts
    (since the last `participate` call) are sent each round.  The session's
    own history already contains earlier posts, so the LLM still has the
    full picture.
    """

    def __init__(
        self,
        name: str,
        persona: str,
        topic_id: str,
        user_id: str,
        temperature: float = 0.7,
        bot_base_url: str | None = None,
        enabled_tools: list[str] | None = None,
    ):
        self.name = name
        self.persona = persona
        self.topic_id = topic_id
        self.temperature = temperature

        port = os.getenv("PORT_AGENT", "51200")
        self._bot_url = (bot_base_url or f"http://127.0.0.1:{port}") + "/v1/chat/completions"

        # Auth: INTERNAL_TOKEN:<user_id> â€” admin-level, session owned by user
        self._user_id = user_id
        self._internal_token = os.getenv("INTERNAL_TOKEN", "")

        # Unique session_id per expert per topic
        safe_name = name.replace(" ", "_")
        self.session_id = f"oasis_{topic_id}_{safe_name}"

        self.enabled_tools = enabled_tools
        self._initialized = False
        self._seen_post_ids: set[int] = set()  # Track which posts we've already sent

    def _auth_header(self) -> dict:
        """Build auth header as INTERNAL_TOKEN:user_id (admin-level, no password needed)."""
        return {"Authorization": f"Bearer {self._internal_token}:{self._user_id}"}

    async def participate(self, forum: DiscussionForum):
        """
        Participate in one round of discussion via bot session.

        First call sends full context + persona instruction.
        Subsequent calls send only NEW posts (incremental delta).
        The bot session's checkpoint history retains earlier context.
        """
        others = await forum.browse(viewer=self.name, exclude_self=True)

        # Split into already-seen vs new posts
        new_posts = [p for p in others if p.id not in self._seen_post_ids]
        self._seen_post_ids.update(p.id for p in others)

        # Build OpenAI-compatible request
        messages = []
        if not self._initialized:
            # â”€â”€ First round: full context â”€â”€
            posts_text = _format_posts(others) if others else "(è¿˜æ²¡æœ‰å…¶ä»–äººå‘è¨€ï¼Œä½ æ¥å¼€å¯è®¨è®ºå§)"
            prompt = _build_discuss_prompt(self.name, self.persona, forum.question, posts_text)

            messages.append({
                "role": "system",
                "content": (
                    f"ä½ æ˜¯è®ºå›ä¸“å®¶ã€Œ{self.name}ã€ã€‚{self.persona}\n"
                    "åœ¨æ¥ä¸‹æ¥çš„è®¨è®ºä¸­ï¼Œä½ å°†æ”¶åˆ°è®ºå›çš„æ–°å¢å†…å®¹ï¼Œéœ€è¦ä»¥ JSON æ ¼å¼å›å¤ä½ çš„è§‚ç‚¹å’ŒæŠ•ç¥¨ã€‚\n"
                    "ä½ æ‹¥æœ‰å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå¦‚éœ€æœç´¢èµ„æ–™ã€åˆ†ææ•°æ®æ¥æ”¯æ’‘ä½ çš„è§‚ç‚¹ï¼Œå¯ä»¥ä½¿ç”¨å¯ç”¨çš„å·¥å…·ã€‚\n"
                    "æ³¨æ„ï¼šåç»­è½®æ¬¡åªä¼šå‘é€æ–°å¢å¸–å­ï¼Œä¹‹å‰çš„å¸–å­è¯·å‚è€ƒä½ çš„å¯¹è¯è®°å¿†ã€‚"
                ),
            })
            messages.append({"role": "user", "content": prompt})
            self._initialized = True
        else:
            # â”€â”€ Subsequent rounds: incremental delta only â”€â”€
            if new_posts:
                new_text = _format_posts(new_posts)
                prompt = (
                    f"ã€ç¬¬ {forum.current_round} è½®è®¨è®ºæ›´æ–°ã€‘\n"
                    f"ä»¥ä¸‹æ˜¯è‡ªä½ ä¸Šæ¬¡å‘è¨€åçš„ {len(new_posts)} æ¡æ–°å¸–å­ï¼š\n\n"
                    f"{new_text}\n\n"
                    "è¯·åŸºäºè¿™äº›æ–°è§‚ç‚¹ä»¥åŠä½ ä¹‹å‰çœ‹åˆ°çš„è®¨è®ºå†…å®¹ï¼Œä»¥ JSON æ ¼å¼å›å¤ï¼š\n"
                    "{\n"
                    '  "reply_to": <æŸä¸ªå¸–å­ID>,\n'
                    '  "content": "ä½ çš„è§‚ç‚¹ï¼ˆ200å­—ä»¥å†…ï¼‰",\n'
                    '  "votes": [{"post_id": <ID>, "direction": "upæˆ–down"}]\n'
                    "}"
                )
            else:
                prompt = (
                    f"ã€ç¬¬ {forum.current_round} è½®è®¨è®ºæ›´æ–°ã€‘\n"
                    "æœ¬è½®æ²¡æœ‰æ–°çš„å¸–å­ã€‚å¦‚æœä½ æœ‰æ–°çš„æƒ³æ³•æˆ–è¡¥å……ï¼Œå¯ä»¥ç»§ç»­å‘è¨€ï¼›"
                    "å¦‚æœæ²¡æœ‰ï¼Œå›å¤ä¸€ä¸ªç©º content å³å¯ã€‚\n"
                    "{\n"
                    '  "reply_to": null,\n'
                    '  "content": "",\n'
                    '  "votes": []\n'
                    "}"
                )
            messages.append({"role": "user", "content": prompt})

        body: dict = {
            "model": "mini-timebot",
            "messages": messages,
            "stream": False,
            "session_id": self.session_id,
        }
        if self.enabled_tools is not None:
            body["enabled_tools"] = self.enabled_tools

        try:
            async with httpx.AsyncClient(timeout=httpx.Timeout(timeout=120.0)) as client:
                resp = await client.post(
                    self._bot_url,
                    json=body,
                    headers=self._auth_header(),
                )

            if resp.status_code != 200:
                print(f"  [OASIS] âŒ {self.name} bot API error {resp.status_code}: {resp.text[:200]}")
                return

            data = resp.json()
            raw_content = data["choices"][0]["message"]["content"]
            result = _parse_expert_response(raw_content)
            await _apply_response(result, self.name, forum, others)

        except json.JSONDecodeError as e:
            print(f"  [OASIS] âš ï¸ {self.name} JSON parse error: {e}")
            try:
                await forum.publish(author=self.name, content=raw_content.strip()[:300])
            except Exception:
                pass
        except Exception as e:
            print(f"  [OASIS] âŒ {self.name} error: {e}")


