"""
OASIS Forum - Discussion Engine

Manages the full lifecycle of a discussion:
  Round loop -> scheduled/parallel expert participation -> consensus check -> summarize

Supports two modes:
  1. Default: all experts participate in parallel each round (original behavior)
  2. Scheduled: follow a YAML schedule that defines speaking order per step
"""

import asyncio
import os

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

from oasis.forum import DiscussionForum
from oasis.experts import ExpertAgent, BotSessionExpert, EXPERT_CONFIGS, get_all_experts
from oasis.scheduler import Schedule, ScheduleStep, StepType, parse_schedule, load_schedule_file

# åŠ è½½æ€»ç»“ prompt æ¨¡æ¿ï¼ˆæ¨¡å—çº§åˆ«ï¼Œå¯¼å…¥æ—¶æ‰§è¡Œä¸€æ¬¡ï¼‰
_prompts_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "prompts")
_summary_tpl_path = os.path.join(_prompts_dir, "oasis_summary.txt")
try:
    with open(_summary_tpl_path, "r", encoding="utf-8") as f:
        _SUMMARY_PROMPT_TPL = f.read().strip()
    print("[prompts] âœ… oasis å·²åŠ è½½ oasis_summary.txt")
except FileNotFoundError:
    print(f"[prompts] âš ï¸ æœªæ‰¾åˆ° {_summary_tpl_path}ï¼Œä½¿ç”¨å†…ç½®é»˜è®¤æ¨¡æ¿")
    _SUMMARY_PROMPT_TPL = ""


def _get_summarizer() -> ChatOpenAI:
    """Create a low-temperature LLM for reliable summarization."""
    api_key = os.getenv("LLM_API_KEY")
    if not api_key:
        raise ValueError("LLM_API_KEY not found in environment variables.")
    base_url = os.getenv("LLM_BASE_URL", "https://api.deepseek.com").strip()
    # ChatOpenAI éœ€è¦ /v1 è·¯å¾„
    openai_base = base_url.rstrip("/") + "/v1"
    return ChatOpenAI(
        model=os.getenv("LLM_MODEL", "deepseek-chat"),
        base_url=openai_base,
        api_key=api_key,
        temperature=0.3,
        max_tokens=2048,
        timeout=60,
        max_retries=2,
    )


class DiscussionEngine:
    """
    Orchestrates one complete discussion session.
    
    Flow:
      1. If schedule is provided, execute steps in defined order
      2. Otherwise, all selected experts participate in parallel each round
      3. After each round, check if consensus is reached
      4. When done (consensus or max rounds), summarize top posts into conclusion
    """

    def __init__(
        self,
        forum: DiscussionForum,
        expert_tags: list[str] | None = None,
        schedule: Schedule | None = None,
        schedule_yaml: str | None = None,
        schedule_file: str | None = None,
        use_bot_session: bool = False,
        bot_base_url: str | None = None,
        bot_enabled_tools: list[str] | None = None,
        user_id: str = "anonymous",
    ):
        self.forum = forum
        self.use_bot_session = use_bot_session

        # Merge public + user custom experts, then filter by tag
        all_configs = get_all_experts(user_id)
        configs = all_configs
        if expert_tags:
            configs = [c for c in all_configs if c["tag"] in expert_tags]
        if not configs:
            configs = all_configs  # Fallback: use all if no match

        if use_bot_session:
            # Backend 2: each expert = a bot session owned by the requesting user
            self.experts: list[ExpertAgent | BotSessionExpert] = [
                BotSessionExpert(
                    name=c["name"],
                    persona=c["persona"],
                    topic_id=forum.topic_id,
                    user_id=user_id,
                    temperature=c["temperature"],
                    bot_base_url=bot_base_url,
                    enabled_tools=bot_enabled_tools,
                )
                for c in configs
            ]
        else:
            # Backend 1: direct LLM (original)
            self.experts = [
                ExpertAgent(
                    name=c["name"],
                    persona=c["persona"],
                    temperature=c["temperature"],
                )
                for c in configs
            ]

        # Build name -> Expert lookup
        self._expert_map: dict[str, ExpertAgent | BotSessionExpert] = {
            e.name: e for e in self.experts
        }

        self.summarizer = _get_summarizer()

        # Load schedule (priority: direct object > yaml string > file path)
        self.schedule: Schedule | None = None
        if schedule:
            self.schedule = schedule
        elif schedule_yaml:
            self.schedule = parse_schedule(schedule_yaml)
        elif schedule_file:
            self.schedule = load_schedule_file(schedule_file)

    def _resolve_experts(self, names: list[str]) -> list[ExpertAgent]:
        """Resolve expert names to ExpertAgent objects. Skip unknown names."""
        resolved = []
        for name in names:
            agent = self._expert_map.get(name)
            if agent:
                resolved.append(agent)
            else:
                print(f"  [OASIS] âš ï¸ Schedule references unknown expert: '{name}', skipping")
        return resolved

    async def run(self):
        """Run the full discussion loop (called as a background task)."""
        self.forum.status = "discussing"

        backend = "bot_session" if self.use_bot_session else "direct_llm"
        mode = "scheduled" if self.schedule else "parallel"
        print(
            f"[OASIS] ğŸ›ï¸ Discussion started: {self.forum.topic_id} "
            f"({len(self.experts)} experts, max {self.forum.max_rounds} rounds, "
            f"mode={mode}, backend={backend})"
        )

        try:
            if self.schedule:
                await self._run_scheduled()
            else:
                await self._run_parallel()

            # Generate final conclusion
            self.forum.conclusion = await self._summarize()
            self.forum.status = "concluded"
            print(f"[OASIS] âœ… Discussion concluded: {self.forum.topic_id}")

        except Exception as e:
            print(f"[OASIS] âŒ Discussion error: {e}")
            self.forum.status = "error"
            self.forum.conclusion = f"è®¨è®ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"

    async def _run_parallel(self):
        """Original behavior: all experts in parallel each round."""
        for round_num in range(self.forum.max_rounds):
            self.forum.current_round = round_num + 1
            print(f"[OASIS] ğŸ“¢ Round {self.forum.current_round}/{self.forum.max_rounds}")

            await asyncio.gather(
                *[expert.participate(self.forum) for expert in self.experts],
                return_exceptions=True,
            )

            if round_num >= 1 and await self._consensus_reached():
                print(f"[OASIS] ğŸ¤ Consensus reached at round {self.forum.current_round}")
                break

    async def _run_scheduled(self):
        """
        Execute the schedule.

        Two modes controlled by schedule.repeat:
          repeat=true  -> Each round executes the full plan, up to max_rounds.
          repeat=false -> All steps execute once sequentially; each step = 1 round.
        """
        steps = self.schedule.steps

        if self.schedule.repeat:
            # â”€â”€ repeat mode: plan æ¯è½®é‡å¤ â”€â”€
            for round_num in range(self.forum.max_rounds):
                self.forum.current_round = round_num + 1
                print(f"[OASIS] ğŸ“¢ Round {self.forum.current_round}/{self.forum.max_rounds}")

                for step in steps:
                    await self._execute_step(step)

                if round_num >= 1 and await self._consensus_reached():
                    print(f"[OASIS] ğŸ¤ Consensus reached at round {self.forum.current_round}")
                    break
        else:
            # â”€â”€ once mode: æ­¥éª¤é¡ºåºæ‰§è¡Œä¸€æ¬¡ï¼Œæ¯æ­¥ç®—ä¸€è½® â”€â”€
            for step_idx, step in enumerate(steps):
                self.forum.current_round = step_idx + 1
                self.forum.max_rounds = len(steps)  # è®©å‰ç«¯æ˜¾ç¤ºæ­£ç¡®çš„æ€»è½®æ•°
                print(f"[OASIS] ğŸ“¢ Step {step_idx + 1}/{len(steps)}")

                await self._execute_step(step)

                if step_idx >= 1 and await self._consensus_reached():
                    print(f"[OASIS] ğŸ¤ Consensus reached at step {step_idx + 1}")
                    break

    async def _execute_step(self, step: ScheduleStep):
        """Execute a single schedule step."""
        if step.step_type == StepType.MANUAL:
            print(f"  [OASIS] ğŸ“ Manual post by {step.manual_author}")
            await self.forum.publish(
                author=step.manual_author,
                content=step.manual_content,
                reply_to=step.manual_reply_to,
            )

        elif step.step_type == StepType.ALL:
            print(f"  [OASIS] ğŸ‘¥ All experts speak")
            await asyncio.gather(
                *[expert.participate(self.forum) for expert in self.experts],
                return_exceptions=True,
            )

        elif step.step_type == StepType.EXPERT:
            agents = self._resolve_experts(step.expert_names)
            if agents:
                print(f"  [OASIS] ğŸ¤ {agents[0].name} speaks")
                await agents[0].participate(self.forum)

        elif step.step_type == StepType.PARALLEL:
            agents = self._resolve_experts(step.expert_names)
            if agents:
                names = ", ".join(a.name for a in agents)
                print(f"  [OASIS] ğŸ¤ Parallel: {names}")
                await asyncio.gather(
                    *[agent.participate(self.forum) for agent in agents],
                    return_exceptions=True,
                )

    async def _consensus_reached(self) -> bool:
        """Check if the top post has enough agreement to stop early."""
        top = await self.forum.get_top_posts(1)
        if not top:
            return False
        # Consensus = top post has >= 70% upvotes from all experts
        threshold = len(self.experts) * 0.7
        return top[0].upvotes >= threshold

    async def _summarize(self) -> str:
        """Summarize the top-voted posts into a final conclusion."""
        top_posts = await self.forum.get_top_posts(5)
        all_posts = await self.forum.browse()

        if not top_posts:
            return "è®¨è®ºæœªäº§ç”Ÿæœ‰æ•ˆè§‚ç‚¹ã€‚"

        posts_text = "\n".join([
            f"[ğŸ‘{p.upvotes} ğŸ‘{p.downvotes}] {p.author}: {p.content}"
            for p in top_posts
        ])

        if _SUMMARY_PROMPT_TPL:
            prompt = _SUMMARY_PROMPT_TPL.format(
                question=self.forum.question,
                post_count=len(all_posts),
                round_count=self.forum.current_round,
                posts_text=posts_text,
            )
        else:
            prompt = (
                f"ä½ æ˜¯ä¸€ä¸ªè®¨è®ºæ€»ç»“ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯å…³äºã€Œ{self.forum.question}ã€çš„å¤šä¸“å®¶è®¨è®ºç»“æœã€‚\n\n"
                f"å…± {len(all_posts)} æ¡å¸–å­ï¼Œç»è¿‡ {self.forum.current_round} è½®è®¨è®ºã€‚\n\n"
                f"è·å¾—æœ€é«˜è®¤å¯çš„è§‚ç‚¹:\n{posts_text}\n\n"
                "è¯·ç»¼åˆä»¥ä¸Šé«˜èµè§‚ç‚¹ï¼Œç»™å‡ºä¸€ä¸ªå…¨é¢ã€å¹³è¡¡ã€æœ‰ç»“è®ºæ€§çš„æœ€ç»ˆå›ç­”ï¼ˆ300å­—ä»¥å†…ï¼‰ã€‚\n"
                "è¦æ±‚:\n"
                "1. æ¸…æ™°æ¦‚æ‹¬å„æ–¹æ ¸å¿ƒè§‚ç‚¹\n"
                "2. æŒ‡å‡ºä¸»è¦å…±è¯†å’Œåˆ†æ­§\n"
                "3. ç»™å‡ºæ˜ç¡®çš„ç»“è®ºæ€§å»ºè®®\n"
            )

        try:
            resp = await self.summarizer.ainvoke([HumanMessage(content=prompt)])
            return resp.content
        except Exception as e:
            return f"æ€»ç»“ç”Ÿæˆå¤±è´¥: {str(e)}"
